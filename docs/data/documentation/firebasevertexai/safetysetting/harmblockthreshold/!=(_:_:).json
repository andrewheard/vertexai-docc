{"metadata":{"roleHeading":"Operator","modules":[{"name":"FirebaseVertexAI","relatedModules":["Swift"]}],"extendedModule":"Swift","title":"!=(_:_:)","externalID":"s:SQsE2neoiySbx_xtFZ::SYNTHESIZED::s:16FirebaseVertexAI13SafetySettingV18HarmBlockThresholdO","fragments":[{"text":"static","kind":"keyword"},{"kind":"text","text":" "},{"kind":"keyword","text":"func"},{"text":" ","kind":"text"},{"text":"!=","kind":"identifier"},{"kind":"text","text":" "},{"text":"(","kind":"text"},{"text":"Self","kind":"typeIdentifier"},{"text":", ","kind":"text"},{"text":"Self","kind":"typeIdentifier"},{"text":") -> ","kind":"text"},{"kind":"typeIdentifier","preciseIdentifier":"s:Sb","text":"Bool"}],"role":"symbol","symbolKind":"op"},"kind":"symbol","schemaVersion":{"major":0,"minor":3,"patch":0},"identifier":{"url":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold\/!=(_:_:)","interfaceLanguage":"swift"},"sections":[],"abstract":[{"text":"Inherited from ","type":"text"},{"type":"codeVoice","code":"Equatable.!=(_:_:)"},{"type":"text","text":"."}],"primaryContentSections":[{"declarations":[{"tokens":[{"text":"static","kind":"keyword"},{"text":" ","kind":"text"},{"kind":"keyword","text":"func"},{"kind":"text","text":" "},{"kind":"identifier","text":"!="},{"text":" ","kind":"text"},{"kind":"text","text":"("},{"kind":"internalParam","text":"lhs"},{"text":": ","kind":"text"},{"kind":"typeIdentifier","text":"Self"},{"text":", ","kind":"text"},{"kind":"internalParam","text":"rhs"},{"kind":"text","text":": "},{"kind":"typeIdentifier","text":"Self"},{"kind":"text","text":") -> "},{"text":"Bool","kind":"typeIdentifier","preciseIdentifier":"s:Sb"}],"languages":["swift"],"platforms":["macOS"]}],"kind":"declarations"}],"variants":[{"paths":["\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold\/!=(_:_:)"],"traits":[{"interfaceLanguage":"swift"}]}],"hierarchy":{"paths":[["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI","doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting","doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold","doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold\/Equatable-Implementations"]]},"references":{"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/HarmBlockThreshold":{"role":"symbol","type":"topic","fragments":[{"text":"enum","kind":"keyword"},{"kind":"text","text":" "},{"kind":"identifier","text":"HarmBlockThreshold"}],"url":"\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold","navigatorTitle":[{"kind":"identifier","text":"HarmBlockThreshold"}],"abstract":[{"type":"text","text":"Block at and beyond a specified "},{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetyRating\/HarmProbability","isActive":true,"type":"reference"},{"text":".","type":"text"}],"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold","title":"SafetySetting.HarmBlockThreshold"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetyRating/HarmProbability":{"title":"SafetyRating.HarmProbability","navigatorTitle":[{"text":"HarmProbability","kind":"identifier"}],"url":"\/documentation\/firebasevertexai\/safetyrating\/harmprobability","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetyRating\/HarmProbability","kind":"symbol","abstract":[{"type":"text","text":"The probability that a given model output falls under a harmful content category. This does"},{"type":"text","text":" "},{"text":"not indicate the severity of harm for a piece of content.","type":"text"}],"role":"symbol","fragments":[{"kind":"keyword","text":"enum"},{"text":" ","kind":"text"},{"text":"HarmProbability","kind":"identifier"}],"type":"topic"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/HarmBlockThreshold/!=(_:_:)":{"role":"symbol","fragments":[{"text":"static","kind":"keyword"},{"text":" ","kind":"text"},{"text":"func","kind":"keyword"},{"text":" ","kind":"text"},{"kind":"identifier","text":"!="},{"kind":"text","text":" "},{"kind":"text","text":"("},{"kind":"typeIdentifier","text":"Self"},{"text":", ","kind":"text"},{"kind":"typeIdentifier","text":"Self"},{"text":") -> ","kind":"text"},{"preciseIdentifier":"s:Sb","kind":"typeIdentifier","text":"Bool"}],"title":"!=(_:_:)","abstract":[],"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold\/!=(_:_:)","kind":"symbol","type":"topic","url":"\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold\/!=(_:_:)"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting":{"role":"symbol","type":"topic","fragments":[{"kind":"keyword","text":"struct"},{"text":" ","kind":"text"},{"kind":"identifier","text":"SafetySetting"}],"url":"\/documentation\/firebasevertexai\/safetysetting","navigatorTitle":[{"text":"SafetySetting","kind":"identifier"}],"abstract":[{"text":"A type used to specify a threshold for harmful content, beyond which the model will return a","type":"text"},{"type":"text","text":" "},{"type":"text","text":"fallback response instead of generated content."}],"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting","title":"SafetySetting"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/HarmBlockThreshold/Equatable-Implementations":{"kind":"article","abstract":[],"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold\/Equatable-Implementations","title":"Equatable Implementations","role":"collectionGroup","url":"\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold\/equatable-implementations","type":"topic"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI":{"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI","type":"topic","url":"\/documentation\/firebasevertexai","abstract":[],"title":"FirebaseVertexAI","role":"collection"}}}