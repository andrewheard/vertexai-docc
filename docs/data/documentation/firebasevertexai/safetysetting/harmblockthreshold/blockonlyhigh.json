{"kind":"symbol","variants":[{"paths":["\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold\/blockonlyhigh"],"traits":[{"interfaceLanguage":"swift"}]}],"schemaVersion":{"patch":0,"major":0,"minor":3},"metadata":{"title":"SafetySetting.HarmBlockThreshold.blockOnlyHigh","externalID":"s:16FirebaseVertexAI13SafetySettingV18HarmBlockThresholdO13blockOnlyHighyA2EmF","symbolKind":"case","role":"symbol","platforms":[{"name":"iOS","unavailable":false,"introducedAt":"15.0","beta":false,"deprecated":false},{"deprecated":false,"unavailable":false,"beta":false,"name":"macOS","introducedAt":"11.0"},{"beta":false,"introducedAt":"15.0","name":"Mac Catalyst","deprecated":false,"unavailable":false},{"beta":false,"introducedAt":"15.0","unavailable":false,"deprecated":false,"name":"tvOS"},{"beta":false,"deprecated":false,"introducedAt":"8.0","unavailable":false,"name":"watchOS"}],"fragments":[{"text":"case","kind":"keyword"},{"text":" ","kind":"text"},{"text":"blockOnlyHigh","kind":"identifier"}],"roleHeading":"Case","modules":[{"name":"FirebaseVertexAI"}]},"hierarchy":{"paths":[["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI","doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting","doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold"]]},"abstract":[{"text":"Content with ","type":"text"},{"code":".negligible","type":"codeVoice"},{"text":", ","type":"text"},{"code":".low","type":"codeVoice"},{"text":", and ","type":"text"},{"code":".medium","type":"codeVoice"},{"text":" will be allowed.","type":"text"}],"primaryContentSections":[{"declarations":[{"platforms":["macOS"],"tokens":[{"text":"case","kind":"keyword"},{"kind":"text","text":" "},{"kind":"identifier","text":"blockOnlyHigh"}],"languages":["swift"]}],"kind":"declarations"}],"sections":[],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold\/blockOnlyHigh"},"references":{"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/HarmBlockThreshold/blockOnlyHigh":{"kind":"symbol","fragments":[{"text":"case","kind":"keyword"},{"kind":"text","text":" "},{"kind":"identifier","text":"blockOnlyHigh"}],"abstract":[{"text":"Content with ","type":"text"},{"type":"codeVoice","code":".negligible"},{"type":"text","text":", "},{"type":"codeVoice","code":".low"},{"type":"text","text":", and "},{"type":"codeVoice","code":".medium"},{"type":"text","text":" will be allowed."}],"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold\/blockOnlyHigh","title":"SafetySetting.HarmBlockThreshold.blockOnlyHigh","role":"symbol","url":"\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold\/blockonlyhigh","type":"topic"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetyRating/HarmProbability":{"title":"SafetyRating.HarmProbability","navigatorTitle":[{"text":"HarmProbability","kind":"identifier"}],"url":"\/documentation\/firebasevertexai\/safetyrating\/harmprobability","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetyRating\/HarmProbability","kind":"symbol","abstract":[{"type":"text","text":"The probability that a given model output falls under a harmful content category. This does"},{"type":"text","text":" "},{"text":"not indicate the severity of harm for a piece of content.","type":"text"}],"role":"symbol","fragments":[{"kind":"keyword","text":"enum"},{"text":" ","kind":"text"},{"text":"HarmProbability","kind":"identifier"}],"type":"topic"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/HarmBlockThreshold":{"role":"symbol","type":"topic","fragments":[{"text":"enum","kind":"keyword"},{"kind":"text","text":" "},{"kind":"identifier","text":"HarmBlockThreshold"}],"url":"\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold","navigatorTitle":[{"kind":"identifier","text":"HarmBlockThreshold"}],"abstract":[{"type":"text","text":"Block at and beyond a specified "},{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetyRating\/HarmProbability","isActive":true,"type":"reference"},{"text":".","type":"text"}],"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold","title":"SafetySetting.HarmBlockThreshold"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI":{"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI","type":"topic","url":"\/documentation\/firebasevertexai","abstract":[],"title":"FirebaseVertexAI","role":"collection"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting":{"role":"symbol","type":"topic","fragments":[{"kind":"keyword","text":"struct"},{"text":" ","kind":"text"},{"kind":"identifier","text":"SafetySetting"}],"url":"\/documentation\/firebasevertexai\/safetysetting","navigatorTitle":[{"text":"SafetySetting","kind":"identifier"}],"abstract":[{"text":"A type used to specify a threshold for harmful content, beyond which the model will return a","type":"text"},{"type":"text","text":" "},{"type":"text","text":"fallback response instead of generated content."}],"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting","title":"SafetySetting"}}}