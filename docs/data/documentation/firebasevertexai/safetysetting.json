{"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/firebasevertexai\/safetysetting"]}],"primaryContentSections":[{"kind":"declarations","declarations":[{"languages":["swift"],"tokens":[{"kind":"keyword","text":"struct"},{"text":" ","kind":"text"},{"text":"SafetySetting","kind":"identifier"}],"platforms":["macOS"]}]}],"metadata":{"fragments":[{"text":"struct","kind":"keyword"},{"kind":"text","text":" "},{"kind":"identifier","text":"SafetySetting"}],"role":"symbol","modules":[{"name":"FirebaseVertexAI"}],"title":"SafetySetting","symbolKind":"struct","roleHeading":"Structure","platforms":[{"unavailable":false,"name":"iOS","beta":false,"deprecated":false,"introducedAt":"15.0"},{"introducedAt":"11.0","deprecated":false,"beta":false,"name":"macOS","unavailable":false},{"introducedAt":"15.0","name":"Mac Catalyst","beta":false,"deprecated":false,"unavailable":false},{"introducedAt":"15.0","unavailable":false,"deprecated":false,"beta":false,"name":"tvOS"},{"deprecated":false,"unavailable":false,"beta":false,"introducedAt":"8.0","name":"watchOS"}],"externalID":"s:16FirebaseVertexAI13SafetySettingV","navigatorTitle":[{"kind":"identifier","text":"SafetySetting"}]},"topicSections":[{"identifiers":["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/init(harmCategory:threshold:)"],"title":"Initializers"},{"identifiers":["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/harmCategory","doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/threshold"],"title":"Instance Properties"},{"title":"Enumerations","identifiers":["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold"]}],"relationshipsSections":[{"kind":"relationships","identifiers":["doc:\/\/FirebaseVertexAI\/SE"],"type":"conformsTo","title":"Conforms To"}],"identifier":{"url":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting","interfaceLanguage":"swift"},"hierarchy":{"paths":[["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI"]]},"schemaVersion":{"major":0,"minor":3,"patch":0},"kind":"symbol","abstract":[{"text":"A type used to specify a threshold for harmful content, beyond which the model will return a","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"fallback response instead of generated content."}],"sections":[],"references":{"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/HarmBlockThreshold":{"role":"symbol","type":"topic","fragments":[{"text":"enum","kind":"keyword"},{"kind":"text","text":" "},{"kind":"identifier","text":"HarmBlockThreshold"}],"url":"\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold","navigatorTitle":[{"kind":"identifier","text":"HarmBlockThreshold"}],"abstract":[{"type":"text","text":"Block at and beyond a specified "},{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetyRating\/HarmProbability","isActive":true,"type":"reference"},{"text":".","type":"text"}],"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold","title":"SafetySetting.HarmBlockThreshold"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/init(harmCategory:threshold:)":{"role":"symbol","type":"topic","fragments":[{"kind":"identifier","text":"init"},{"kind":"text","text":"("},{"kind":"externalParam","text":"harmCategory"},{"kind":"text","text":": "},{"preciseIdentifier":"s:16FirebaseVertexAI12HarmCategoryO","kind":"typeIdentifier","text":"HarmCategory"},{"text":", ","kind":"text"},{"text":"threshold","kind":"externalParam"},{"kind":"text","text":": "},{"kind":"typeIdentifier","preciseIdentifier":"s:16FirebaseVertexAI13SafetySettingV","text":"SafetySetting"},{"kind":"text","text":"."},{"preciseIdentifier":"s:16FirebaseVertexAI13SafetySettingV18HarmBlockThresholdO","text":"HarmBlockThreshold","kind":"typeIdentifier"},{"kind":"text","text":")"}],"url":"\/documentation\/firebasevertexai\/safetysetting\/init(harmcategory:threshold:)","abstract":[{"text":"Initializes a new safety setting with the given category and threshold.","type":"text"}],"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/init(harmCategory:threshold:)","title":"init(harmCategory:threshold:)"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/harmCategory":{"role":"symbol","type":"topic","fragments":[{"kind":"keyword","text":"let"},{"text":" ","kind":"text"},{"kind":"identifier","text":"harmCategory"},{"kind":"text","text":": "},{"preciseIdentifier":"s:16FirebaseVertexAI12HarmCategoryO","kind":"typeIdentifier","text":"HarmCategory"}],"url":"\/documentation\/firebasevertexai\/safetysetting\/harmcategory","abstract":[{"text":"The category this safety setting should be applied to.","type":"text"}],"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/harmCategory","title":"harmCategory"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting":{"role":"symbol","type":"topic","fragments":[{"kind":"keyword","text":"struct"},{"text":" ","kind":"text"},{"kind":"identifier","text":"SafetySetting"}],"url":"\/documentation\/firebasevertexai\/safetysetting","navigatorTitle":[{"text":"SafetySetting","kind":"identifier"}],"abstract":[{"text":"A type used to specify a threshold for harmful content, beyond which the model will return a","type":"text"},{"type":"text","text":" "},{"type":"text","text":"fallback response instead of generated content."}],"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting","title":"SafetySetting"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetyRating/HarmProbability":{"title":"SafetyRating.HarmProbability","navigatorTitle":[{"text":"HarmProbability","kind":"identifier"}],"url":"\/documentation\/firebasevertexai\/safetyrating\/harmprobability","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetyRating\/HarmProbability","kind":"symbol","abstract":[{"type":"text","text":"The probability that a given model output falls under a harmful content category. This does"},{"type":"text","text":" "},{"text":"not indicate the severity of harm for a piece of content.","type":"text"}],"role":"symbol","fragments":[{"kind":"keyword","text":"enum"},{"text":" ","kind":"text"},{"text":"HarmProbability","kind":"identifier"}],"type":"topic"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/threshold":{"type":"topic","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/threshold","role":"symbol","title":"threshold","fragments":[{"kind":"keyword","text":"let"},{"text":" ","kind":"text"},{"text":"threshold","kind":"identifier"},{"kind":"text","text":": "},{"preciseIdentifier":"s:16FirebaseVertexAI13SafetySettingV","text":"SafetySetting","kind":"typeIdentifier"},{"kind":"text","text":"."},{"preciseIdentifier":"s:16FirebaseVertexAI13SafetySettingV18HarmBlockThresholdO","text":"HarmBlockThreshold","kind":"typeIdentifier"}],"kind":"symbol","abstract":[{"type":"text","text":"The threshold describing what content should be blocked."}],"url":"\/documentation\/firebasevertexai\/safetysetting\/threshold"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI":{"kind":"symbol","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI","type":"topic","url":"\/documentation\/firebasevertexai","abstract":[],"title":"FirebaseVertexAI","role":"collection"},"doc://FirebaseVertexAI/SE":{"title":"Swift.Encodable","identifier":"doc:\/\/FirebaseVertexAI\/SE","type":"unresolvable"}}}