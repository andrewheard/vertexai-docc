{"metadata":{"role":"symbol","title":"SafetySetting","platforms":[{"unavailable":false,"introducedAt":"15.0","beta":false,"name":"iOS","deprecated":false},{"introducedAt":"11.0","name":"macOS","unavailable":false,"beta":false,"deprecated":false},{"deprecated":false,"introducedAt":"15.0","beta":false,"name":"Mac Catalyst","unavailable":false},{"deprecated":false,"unavailable":false,"introducedAt":"15.0","beta":false,"name":"tvOS"},{"introducedAt":"8.0","name":"watchOS","beta":false,"unavailable":false,"deprecated":false}],"roleHeading":"Structure","modules":[{"name":"FirebaseVertexAI"}],"symbolKind":"struct","fragments":[{"text":"struct","kind":"keyword"},{"text":" ","kind":"text"},{"text":"SafetySetting","kind":"identifier"}],"navigatorTitle":[{"kind":"identifier","text":"SafetySetting"}],"externalID":"s:16FirebaseVertexAI13SafetySettingV"},"abstract":[{"text":"A type used to specify a threshold for harmful content, beyond which the model will return a","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"fallback response instead of generated content."}],"topicSections":[{"identifiers":["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/init(harmCategory:threshold:)"],"title":"Initializers"},{"identifiers":["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/harmCategory","doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/threshold"],"title":"Instance Properties"},{"title":"Enumerations","identifiers":["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold"]}],"relationshipsSections":[{"identifiers":["doc:\/\/FirebaseVertexAI\/SE"],"kind":"relationships","type":"conformsTo","title":"Conforms To"}],"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/firebasevertexai\/safetysetting"]}],"primaryContentSections":[{"kind":"declarations","declarations":[{"platforms":["macOS"],"tokens":[{"kind":"keyword","text":"struct"},{"text":" ","kind":"text"},{"text":"SafetySetting","kind":"identifier"}],"languages":["swift"]}]}],"kind":"symbol","schemaVersion":{"minor":3,"major":0,"patch":0},"sections":[],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting"},"hierarchy":{"paths":[["doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI"]]},"references":{"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting":{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting","type":"topic","fragments":[{"text":"struct","kind":"keyword"},{"text":" ","kind":"text"},{"text":"SafetySetting","kind":"identifier"}],"title":"SafetySetting","url":"\/documentation\/firebasevertexai\/safetysetting","abstract":[{"type":"text","text":"A type used to specify a threshold for harmful content, beyond which the model will return a"},{"type":"text","text":" "},{"type":"text","text":"fallback response instead of generated content."}],"navigatorTitle":[{"text":"SafetySetting","kind":"identifier"}],"role":"symbol","kind":"symbol"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI":{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI","abstract":[],"url":"\/documentation\/firebasevertexai","role":"collection","type":"topic","title":"FirebaseVertexAI","kind":"symbol"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/init(harmCategory:threshold:)":{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/init(harmCategory:threshold:)","kind":"symbol","fragments":[{"kind":"identifier","text":"init"},{"text":"(","kind":"text"},{"kind":"externalParam","text":"harmCategory"},{"kind":"text","text":": "},{"preciseIdentifier":"s:16FirebaseVertexAI12HarmCategoryO","text":"HarmCategory","kind":"typeIdentifier"},{"text":", ","kind":"text"},{"kind":"externalParam","text":"threshold"},{"text":": ","kind":"text"},{"kind":"typeIdentifier","text":"SafetySetting","preciseIdentifier":"s:16FirebaseVertexAI13SafetySettingV"},{"text":".","kind":"text"},{"text":"HarmBlockThreshold","kind":"typeIdentifier","preciseIdentifier":"s:16FirebaseVertexAI13SafetySettingV18HarmBlockThresholdO"},{"kind":"text","text":")"}],"type":"topic","role":"symbol","url":"\/documentation\/firebasevertexai\/safetysetting\/init(harmcategory:threshold:)","abstract":[{"text":"Initializes a new safety setting with the given category and threshold.","type":"text"}],"title":"init(harmCategory:threshold:)"},"doc://FirebaseVertexAI/SE":{"title":"Swift.Encodable","identifier":"doc:\/\/FirebaseVertexAI\/SE","type":"unresolvable"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/HarmBlockThreshold":{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/HarmBlockThreshold","type":"topic","fragments":[{"text":"enum","kind":"keyword"},{"text":" ","kind":"text"},{"text":"HarmBlockThreshold","kind":"identifier"}],"title":"SafetySetting.HarmBlockThreshold","url":"\/documentation\/firebasevertexai\/safetysetting\/harmblockthreshold","abstract":[{"text":"Block at and beyond a specified ","type":"text"},{"isActive":true,"type":"reference","identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetyRating\/HarmProbability"},{"type":"text","text":"."}],"navigatorTitle":[{"text":"HarmBlockThreshold","kind":"identifier"}],"role":"symbol","kind":"symbol"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetyRating/HarmProbability":{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetyRating\/HarmProbability","kind":"symbol","abstract":[{"type":"text","text":"The probability that a given model output falls under a harmful content category. This does"},{"type":"text","text":" "},{"text":"not indicate the severity of harm for a piece of content.","type":"text"}],"navigatorTitle":[{"text":"HarmProbability","kind":"identifier"}],"fragments":[{"text":"enum","kind":"keyword"},{"kind":"text","text":" "},{"text":"HarmProbability","kind":"identifier"}],"role":"symbol","url":"\/documentation\/firebasevertexai\/safetyrating\/harmprobability","type":"topic","title":"SafetyRating.HarmProbability"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/threshold":{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/threshold","kind":"symbol","fragments":[{"kind":"keyword","text":"let"},{"text":" ","kind":"text"},{"text":"threshold","kind":"identifier"},{"kind":"text","text":": "},{"preciseIdentifier":"s:16FirebaseVertexAI13SafetySettingV","text":"SafetySetting","kind":"typeIdentifier"},{"kind":"text","text":"."},{"text":"HarmBlockThreshold","kind":"typeIdentifier","preciseIdentifier":"s:16FirebaseVertexAI13SafetySettingV18HarmBlockThresholdO"}],"type":"topic","role":"symbol","url":"\/documentation\/firebasevertexai\/safetysetting\/threshold","abstract":[{"text":"The threshold describing what content should be blocked.","type":"text"}],"title":"threshold"},"doc://FirebaseVertexAI/documentation/FirebaseVertexAI/SafetySetting/harmCategory":{"identifier":"doc:\/\/FirebaseVertexAI\/documentation\/FirebaseVertexAI\/SafetySetting\/harmCategory","kind":"symbol","fragments":[{"text":"let","kind":"keyword"},{"kind":"text","text":" "},{"text":"harmCategory","kind":"identifier"},{"kind":"text","text":": "},{"preciseIdentifier":"s:16FirebaseVertexAI12HarmCategoryO","text":"HarmCategory","kind":"typeIdentifier"}],"type":"topic","role":"symbol","url":"\/documentation\/firebasevertexai\/safetysetting\/harmcategory","abstract":[{"text":"The category this safety setting should be applied to.","type":"text"}],"title":"harmCategory"}}}